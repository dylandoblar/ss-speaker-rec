# Multi-task Learning of Speech Representations for Self-supervised Speaker Recognition

Several recent works have demonstrated that embeddings learned through self-supervision can outperform traditional hand-crafted features such as mel-frequency cepstral coefficients (MFCCs) or log-mel filterbanks on various speech-related tasks. However, the performance of these representations has largely been untested for speaker recognition when used with larger, state-of-the-art models. In this work, we explore the effectiveness of representations learned by the problem-agnostic speech encoder+ (PASE+) on speaker recognition. PASE+ is a self-supervised model that learns speech representations using a multi-task learning framework; it involves an encoder followed by multiple workers that jointly solve different generative tasks. We use speech representations generated by PASE+ as inputs to a self-supervised speaker identification model and compare its performance with the log-mel filterbanks used in the original implementation. In doing so, we intend to show that using speech features from PASE+ can improve speaker recognition performance over traditional hand-crafted acoustic features.

This repository contains the code necessary to train and validate both our baseline model, which uses log-mel filterbanks with a Fast ResNet-34-based architecture, and our PASE+ model, which uses PASE+ representations which are fed into the same Fast ResNet-34-based speaker embedding module as used in the baseline.

## Data
To download and extract VoxCeleb and VoxCeleb2, install the dependencies and follow the data preparation instructions in [`sup_voxtrainer`](https://github.com/dylandoblar/ss-speaker-rec/tree/main/sup_voxtrainer).  Note that there is no need to download and extract the MUSAN noise corpus described in [`voxtrainer`](https://github.com/dylandoblar/ss-speaker-rec/tree/main/voxtrainer).

## Training
To run a training job for the baseline model, refer to the [`voxtrainer` training instructions](https://github.com/dylandoblar/ss-speaker-rec/tree/main/voxtrainer#training-example).  To run a training job for the PASE+ model, first extract the PASE+ embeddings (which will be stored as NumPy arrays) using [`pase/run_pase_voxceleb.py`](https://github.com/dylandoblar/ss-speaker-rec/blob/main/pase/run_pase_voxceleb.py).  Then run the training script in [`voxtrainer`](https://github.com/dylandoblar/ss-speaker-rec/tree/main/voxtrainer) with the PASE+ embeddings.  Plots can be generated for the training loss and validation EER using the log file of the training job with the script [`voxtrainer/generate_plots.py`](https://github.com/dylandoblar/ss-speaker-rec/blob/main/voxtrainer/generate_plots.py).

## Validation
Follow instructions in [`val`](https://github.com/dylandoblar/ss-speaker-rec/tree/main/val) to validate a trained model on the validation set for the VoxSRC2020 challenge.
